{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Halo Mass from a Shear Catalog: Redshift distribution of source galaxies\n",
    "\n",
    "_the LSST-DESC CLMM team_\n",
    "\n",
    "\n",
    "This notebook demonstrates how to use `clmm` to estimate a WL halo mass from observations of a galaxy cluster when source galaxies follow a given distribution (especially Chang. (2013) implemented in clmm). It uses several functionalities of the support `mock_data` module to produce datasets of increasing complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import some standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/payernco/CLMM/examples/support')\n",
    "try: import clmm\n",
    "except:\n",
    "    import notebook_install\n",
    "    notebook_install.install_clmm_pipeline(upgrade=False)\n",
    "    import clmm\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from sampler import fitters\n",
    "from scipy.integrate import quad\n",
    "plt.rcParams['font.family']=['gothambook','gotham','gotham-book','serif']\n",
    "\n",
    "import cluster_toolkit as ct\n",
    "\n",
    "clmm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import `clmm`'s core modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clmm.polaraveraging as pa\n",
    "import clmm.galaxycluster as gc\n",
    "import clmm.modeling as modeling\n",
    "from clmm.modeling import angular_diameter_dist_a1a2\n",
    "\n",
    "from astropy import units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import a support modules for a specific data sets.\n",
    "`clmm` includes support modules that enable the user to generate mock data in a format compatible with `clmm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mock_data as mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making mock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create mock data, we need to define a true cosmology, which is currently done with [`astropy`'s cosmology library](http://docs.astropy.org/en/stable/cosmology/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo = FlatLambdaCDM(H0=70, Om0=0.27, Ob0=0.045)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the Chang. (2013) redshift distribution of galaxies:\n",
    "\n",
    "$$\n",
    "N(z) \\propto z^\\alpha\\exp\\left(-\\left[\\frac{z}{z_0}\\right]^\\beta\\right)\n",
    "$$\n",
    "\n",
    "where parameters are chosen $\\alpha, \\beta, z_0$ = $1.24, 1.01, 0.51$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chang(z): #Chang distribution of source redshift\n",
    "    a = 1.24\n",
    "    b = 1.01\n",
    "    z_0 = 0.51\n",
    "    return np.exp(-(z/z_0)**b)*z**a\n",
    "zinf = 20\n",
    "norm = quad(Chang, 0, zinf)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set some parameters for a mock galaxy cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_m = 1.e15\n",
    "logm = np.log(cluster_m)/np.log(10)\n",
    "concentration = 4\n",
    "cluster_ra = 0.\n",
    "cluster_dec = 0.\n",
    "cluster_z = 0.4\n",
    "sigma = 30 # number of galaxies per arcmin^2\n",
    "Delta = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = quad(Chang,0,zinf)[0]\n",
    "ngals = 15000\n",
    "print('The number of generated galaxies = ' + str(ngals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `mock_data` support module to generate 3 galaxy catalogs:\n",
    "- `ideal_data`: galaxies distributed according to the Chang et al. (2013) redshift distribution.\n",
    "- `noisy_data`: `ideal_data` + photoz errors + shape noise\n",
    "\n",
    "(Galaxies have a minimum redshift defined as $z_{cluster} + 0.1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_data = mock.generate_galaxy_catalog(cluster_m, cluster_z, concentration, cosmo, ngals, Delta,'chang13', zsrc_min = cluster_z + 0.1)\n",
    "noisy_data = mock.generate_galaxy_catalog(cluster_m, cluster_z, concentration, cosmo, ngals, Delta,'chang13', zsrc_min = cluster_z + 0.1,\n",
    "                                  shapenoise=0.05, \n",
    "                                  photoz_sigma_unscaled=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The galaxy catalogs are converted to a `clmm.GalaxyCluster` object and may be saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = \"CL_ideal\"\n",
    "gc_object = clmm.GalaxyCluster(cluster_id, cluster_ra, cluster_dec,\n",
    "                                  cluster_z, ideal_data)\n",
    "gc_object.save('ideal_GC.pkl')\n",
    "\n",
    "cluster_id = \"CL_noisy\"\n",
    "gc_object = clmm.GalaxyCluster(cluster_id, cluster_ra, cluster_dec,\n",
    "                                   cluster_z, noisy_data)\n",
    "gc_object.save('noisy_GC.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any saved clmm.GalaxyCluster object may be read in for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_ideal = clmm.load_cluster('ideal_GC.pkl') # background galaxies distributed according to Chang et al. (2013)\n",
    "cl_noisy = clmm.load_cluster('noisy_GC.pkl') # Chang et al. (2013) + shapenoise + photozerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing shear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clmm.polaraveraging.compute_shear` calculates the tangential and cross shears for each source galaxy in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1, g_t1, g_x1 = cl_ideal.compute_shear(geometry=\"flat\")\n",
    "theta2, g_t2, g_x2 = cl_noisy.compute_shear(geometry=\"flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radially binning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = pa.make_bins(0.2, 4, 15, method='evenlog10width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clmm.polaraveraging.make_shear_profile` evaluates the average shear of the galaxy catalog in bins of radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_ideal = cl_ideal.make_shear_profile(\"radians\", \"Mpc\", bins=bin_edges,cosmo=cosmo, gal_ids_in_bins=True)\n",
    "profile_noisy = cl_noisy.make_shear_profile(\"radians\", \"Mpc\", bins=bin_edges,cosmo=cosmo, gal_ids_in_bins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `clmm.polaraveraging.make_shear_profile` on a `clmm.GalaxyCluster object`, the object acquires the `clmm.GalaxyCluster.profile` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source galaxy redshift distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redshift of galaxies generated by mock data are distributed following the Chang. (2013) Redshift distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_source = np.linspace(cluster_z + 0.1,5,100)\n",
    "norm = quad(Chang, cluster_z + 0.1, 20)[0]\n",
    "Chang_distribution = Chang(z_source)/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(cl_ideal.galcat['z'], density = True, bins = 50, label='histogram')\n",
    "plt.plot(z_source, Chang_distribution, '--y', label='Chang. (2013) redshift distribution')\n",
    "plt.xlabel(r'$z_{src}$', fontsize = 30)\n",
    "plt.ylabel(r'N(z)', fontsize = 20)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlim(0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the halo model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced tangential shear for source galaxies at a given redshift $g_t(\\theta, z_s)$ is returned by the fucntion below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reduced_tangential_shear(r, logm, z_src):\n",
    "    m = 10.**logm\n",
    "    gt_model = clmm.predict_reduced_tangential_shear(r*cosmo.h,\n",
    "                                                         m, concentration,\n",
    "                                                         cluster_z, z_src, cosmo,\n",
    "                                                         delta_mdef=200,\n",
    "                                                         halo_profile_model='nfw')    \n",
    "    return gt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit of Halo mass from galaxy catalog must take account of redshift distribution of source galaxies, where $\\langle g_t(\\theta, z_s)\\rangle \\neq g_t(\\theta, \\langle z_s \\rangle$). The reduced tangential shear that corresponds to a continuous distribution of source galaxy redshift $N(z)$ can be expressed as the following integral, as an average on $N(z)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g_t(\\theta) = \\langle g_t(\\theta, z_s)\\rangle_{z_{cluster}} = \\int_{z_{cluster}}^{+\\infty}dz_sN(z_s)g_t(\\theta, z_s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can built a model based on data, such that: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g_t(\\theta) = \\frac{1}{N(\\theta)}\\sum\\limits_{i = 1}^{N(\\theta)}g_t(\\theta, z_i)\\hspace{1cm}\\mbox{where}\\hspace{1cm}\\ N(\\theta) := i-\\mbox{th galaxies with}\\  \\theta_i \\in [\\theta-  \\Delta\\theta/2,  \\theta + \\Delta\\theta/2[\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $N(\\theta)$ is the number of galaxies wih radius $\\theta_i$ such that $\\theta_i \\in [\\theta - \\Delta\\theta/2, \\theta + \\Delta\\theta/2[$. The interval $[\\theta - \\Delta\\theta/2, \\theta + \\Delta\\theta/2[$ is provided above by radially binning the data with `compute_shear`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data-based model almost equals to reduced tangential shear estimated calculated by `compute_shear` when generated galaxies have no shape-noise, such that each ellipticities at redshift $z_i$ and angular distance $\\theta_i$ is equals to reduced shear at $\\theta_i$ for source redshift $z_i$ $g(\\theta_i, z_i)$. Only remains a difference due to the choice $g_t(\\theta_i, z_i)\\rightarrow g_t(\\theta, z_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reduced_tangential_shear_Chang( radius , logm , data , catalog , profile ): \n",
    "    m = 10**logm\n",
    "    gt_model = []\n",
    "    for i in range(len(radius)):\n",
    "        \n",
    "        r = profile['radius'][i]\n",
    "        galist = profile['gal_id'][i]\n",
    "        z = catalog.galcat['z'][galist]\n",
    "        shear = clmm.predict_reduced_tangential_shear(r*cosmo.h,\n",
    "                                                                         m, concentration,\n",
    "                                                                         cluster_z, z, cosmo,                                                          delta_mdef=200,\n",
    "                                                                         halo_profile_model='nfw')\n",
    "        gt_model.append(np.mean(shear))\n",
    "        \n",
    "    return gt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the reduced tangential shear estimated with `compute_shear` for ideal and noisy data (shape-noise plus redshift error) and associated data-based model `predict_reduced_tangential_shear_Chang`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = profile_ideal['radius']\n",
    "z_average = np.mean(profile_ideal['z'])\n",
    "print(z_average)\n",
    "gt_model = predict_reduced_tangential_shear(r, logm, z_average)\n",
    "gt_model_ideal = predict_reduced_tangential_shear_Chang(r,logm, ideal_data, cl_ideal, profile_ideal)\n",
    "gt_model_noisy = predict_reduced_tangential_shear_Chang(r,logm, noisy_data, cl_noisy, profile_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.title('ideal data', fontsize=20)\n",
    "plt.errorbar(r,profile_ideal['gt'],profile_ideal['gt_err'],c='k',fmt='-', label=r'ideal data, $M_{input}$ = %.1e Msun' % cluster_m)\n",
    "plt.loglog(r,gt_model_ideal,':k',  label='model, $M_{input}$ = %.3e Msun' % cluster_m)\n",
    "plt.loglog(r,gt_model,'-y',  label=r'model $g_t(\\theta, \\langle z_s\\rangle)$, $M_{input}$ = %.3e Msun' % cluster_m)\n",
    "plt.xlabel('r [Mpc]', fontsize = 20)\n",
    "plt.ylabel(r'$g_t$', fontsize = 20)\n",
    "plt.xlim(min(profile_ideal['radius']), max(profile_ideal['radius']))\n",
    "plt.legend(fontsize = 15)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.title('noisy data', fontsize=20)\n",
    "plt.errorbar(r,profile_noisy['gt'],profile_noisy['gt_err'],c='r',fmt='-', label=r'noisy data, $M_{input}$ = %.1e Msun' % cluster_m)\n",
    "plt.loglog(r,gt_model_noisy,':r', label='model, $M_{input}$ = %.3e Msun' % cluster_m)\n",
    "plt.xlabel('r [Mpc]', fontsize = 20)\n",
    "plt.ylabel(r'$g_t$', fontsize = 20)\n",
    "plt.xlim(min(profile_ideal['radius']), max(profile_ideal['radius']))\n",
    "plt.legend(fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a halo mass - taking account for the source redshift distribution in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the best-fit mass using `scipy.optimize.curve_fit`.  We compare estimated mass for noisy and ideal data, moreover we estimate the halo mass considering $g_t(\\theta, \\langle z_s\\rangle)$ that consists in the biaised approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt,pcov = fitters['curve_fit'](lambda r, logm:predict_reduced_tangential_shear_Chang(r, logm, ideal_data, cl_ideal, profile_ideal), \n",
    "                        profile_ideal['radius'], \n",
    "                        profile_ideal['gt'], \n",
    "                        profile_ideal['gt_err'], bounds=[10.,16.])\n",
    "\n",
    "m_est_ideal = 10.**popt[0]\n",
    "m_est_err_ideal =  m_est_ideal * np.sqrt(pcov[0][0]) * np.log(10) \n",
    "\n",
    "popt,pcov = fitters['curve_fit'](lambda r, logm:predict_reduced_tangential_shear(r, logm, np.mean(profile_ideal['z'])), \n",
    "                        profile_ideal['radius'], \n",
    "                        profile_ideal['gt'], \n",
    "                        profile_ideal['gt_err'], bounds=[10.,17.])\n",
    "\n",
    "m_est = 10.**popt[0]\n",
    "m_est_err =  m_est * np.sqrt(pcov[0][0]) * np.log(10) \n",
    "\n",
    "\n",
    "popt,pcov = fitters['curve_fit'](lambda r, logm:predict_reduced_tangential_shear_Chang(r, logm, noisy_data, cl_noisy, profile_noisy), \n",
    "                        profile_noisy['radius'], \n",
    "                        profile_noisy['gt'], \n",
    "                        profile_noisy['gt_err'], bounds=[10.,16.])\n",
    "\n",
    "m_est_noisy = 10.**popt[0]\n",
    "m_est_err_noisy =  m_est_noisy * np.sqrt(pcov[0][0]) * np.log(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best fit mass for ideal data = {m_est_ideal:.2e} +/- {m_est_err_ideal:.2e} Msun')\n",
    "print(f'Best fit mass for noisy data = {m_est_noisy:.2e} +/- {m_est_err_noisy:.2e} Msun')\n",
    "print(' ')\n",
    "print(f'Best fit mass for biaised shear model = {m_est:.2e} +/- {m_est_err:.2e} Msun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization purpose, we calculate the reduced tangential shear predicted by the model with estimated masses fornoisy and ideal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_est_ideal = predict_reduced_tangential_shear_Chang(r,np.log(m_est_ideal)/np.log(10), ideal_data, cl_ideal, profile_ideal)\n",
    "gt_est_noisy = predict_reduced_tangential_shear_Chang(r,np.log(m_est_noisy)/np.log(10), noisy_data, cl_noisy, profile_noisy)\n",
    "gt_est = predict_reduced_tangential_shear(r,np.log(m_est)/np.log(10), z_average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare to tangential shear obtained with theoretical mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=( 20 , 8 ))\n",
    "plt.subplot( 1 , 2 , 1 )\n",
    "plt.title('Shear to mass with ideal data', fontsize=20)\n",
    "plt.errorbar(r,profile_ideal['gt'],profile_ideal['gt_err'],c='k',fmt='^', label=r'ideal data, $M_{input}$ = %.1e Msun' % cluster_m)\n",
    "plt.loglog(r,gt_model_ideal,'-r',  label='model, $M_{input}$ = %.3e Msun' % cluster_m)\n",
    "plt.loglog(r,gt_est_ideal,'--b', label='best fit model, M_fit = %.2e +/- %.2e' % (m_est_ideal, m_est_err_ideal))\n",
    "plt.loglog(r,gt_est,'--g', label=r'best fit model $g_t(\\theta,\\langle z_s \\rangle)$, M_fit = %.2e +/- %.2e' % (m_est, m_est_err))\n",
    "\n",
    "\n",
    "plt.xlabel('r [Mpc]', fontsize = 20)\n",
    "plt.ylabel(r'$g_t$', fontsize = 20)\n",
    "plt.xlim(min(profile_ideal['radius']), max(profile_ideal['radius']))\n",
    "plt.legend(fontsize = 15)\n",
    "\n",
    "\n",
    "plt.subplot( 1 , 2 , 2 )\n",
    "plt.title('Shear to mass with noisy data', fontsize=20)\n",
    "plt.errorbar(r,profile_noisy['gt'],profile_noisy['gt_err'],c='k',fmt=':^', label=r'noisy data, $M_{input}$ = %.1e Msun' % cluster_m)\n",
    "plt.loglog(r,gt_model_noisy,'-r',  label='model, $M_{input}$ = %.3e Msun' % cluster_m)\n",
    "plt.loglog(r,gt_est_noisy,'--b', label='best fit model, M_fit = %.2e +/- %.2e' % (m_est_noisy, m_est_err_noisy))\n",
    "\n",
    "plt.xlabel('r [Mpc]', fontsize = 20)\n",
    "plt.ylabel(r'$g_t$', fontsize = 20)\n",
    "plt.xlim(min(profile_noisy['radius']), max(profile_noisy['radius']))\n",
    "plt.legend(fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that estimated mass by fitting the biaised model is uncorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best fit mass for biaised shear model = {m_est:.2e} +/- {m_est_err:.2e} Msun')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
